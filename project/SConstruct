# SConstruct
# Fetch all Osamu Dazai (person 35) works as XHTML from Aozora Bunko.
# Outputs to out/xhtml/*.html and out/manifest.csv

import os, time, re, json, csv
from pathlib import Path
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

# ---------------- Config ----------------
AUTHOR_PAGE = "https://www.aozora.gr.jp/index_pages/person35.html"
RAW_DIR = Path("out/raw")
RAW_AUTHOR = RAW_DIR / "person35.html"
OUT_XHTML_DIR = Path("out/xhtml")
METADATA_JSON = Path("out/cards_metadata.json")
MANIFEST = Path("out/manifest.csv")
REQUEST_DELAY_SEC = float(os.environ.get("REQUEST_DELAY_SEC", "0.6"))
USER_AGENT = "SCons-Dazai-XHTML/2.0 (+academic use)"
LIST_ONLY = os.environ.get("LIST") == "1"  # LIST=1 to list targets without downloading

try:
    MAX_ITEMS = int(os.environ.get("MAX_ITEMS", "0"))
except ValueError:
    MAX_ITEMS = 0

env = Environment(ENV={'PATH': os.environ.get('PATH','')})

def ensure_dirs():
    for p in [RAW_DIR, OUT_XHTML_DIR, MANIFEST.parent]:
        p.mkdir(parents=True, exist_ok=True)

def slugify(s: str) -> str:
    s = (s or '').strip()
    s = re.sub(r'\s+', '_', s)
    s = re.sub(r'[^\w\- _\u3040-\u30ff\u4e00-\u9faf]', '', s)
    return s[:60] if len(s) > 60 else (s or 'untitled')

def fix_mojibake(s: str) -> str:
    if not s:
        return s
    def has_jp(txt):
        return bool(re.search(r'[\u3040-\u30FF\u4E00-\u9FAF]', txt))
    if has_jp(s):
        return s
    cands = []
    for enc_from, enc_to in [('latin-1','utf-8'), ('latin-1','cp932'), ('cp932','utf-8')]:
        try:
            cands.append(s.encode(enc_from).decode(enc_to))
        except Exception:
            pass
    for c in cands:
        if has_jp(c):
            return c
    return s

# -------- Phase 1: Fetch author page --------
def action_fetch_author(target, source, env):
    ensure_dirs()
    time.sleep(REQUEST_DELAY_SEC)
    r = requests.get(AUTHOR_PAGE, headers={"User-Agent": USER_AGENT}, timeout=60)
    r.raise_for_status()
    Path(str(target[0])).write_bytes(r.content)
    return None

FetchAuthor = Builder(action=Action(action_fetch_author, cmdstr="get-author -> ${TARGET}"))
env.Append(BUILDERS={'FetchAuthor': FetchAuthor})
AuthorPage = env.FetchAuthor(str(RAW_AUTHOR), [])

# -------- Phase 2: Parse metadata --------
def action_parse_metadata(target, source, env):
    html = Path(str(source[0])).read_text(encoding='utf-8', errors='ignore')
    soup = BeautifulSoup(html, 'html.parser')
    rows = []
    for a in soup.select('a[href*="/cards/000035/card"]'):
        href = a.get('href')
        if not href:
            continue
        card_url = urljoin(AUTHOR_PAGE, href)
        title = a.get_text(strip=True)
        rows.append({'title_guess': title, 'card_url': card_url})
    seen = set(); uniq=[]
    for r in rows:
        if r['card_url'] not in seen:
            uniq.append(r); seen.add(r['card_url'])
    if MAX_ITEMS>0:
        uniq = uniq[:MAX_ITEMS]
    Path(str(target[0])).write_text(json.dumps(uniq, ensure_ascii=False, indent=2), encoding='utf-8')
    return None

Metadata = env.Command(str(METADATA_JSON), AuthorPage, Action(action_parse_metadata, cmdstr="parse-author -> metadata"))

# Helper to fetch card page and extract XHTML link
def extract_xhtml_from_card(card_url: str):
    r = requests.get(card_url, headers={"User-Agent": USER_AGENT}, timeout=60)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, 'html.parser')
    title = soup.select_one('h1').get_text(strip=True) if soup.select_one('h1') else ''
    link = None; enc=None
    for tr in soup.find_all('tr'):
        txt = tr.get_text(' ', strip=True)
        if 'XHTML' in txt:
            a = tr.find('a')
            if a and a.get('href') and a.get('href').endswith('.html'):
                link = urljoin(card_url, a['href'])
                m = re.search(r'(ShiftJIS|UTF-8|JIS X 0208)', txt, re.IGNORECASE)
                enc = m.group(1) if m else None
                break
    if not link:
        a = soup.find('a', string=lambda s: s and 'XHTML' in s)
        if a and a.get('href'):
            link = urljoin(card_url, a.get('href'))
    return title, link, enc

def decode_html(raw: bytes) -> str:
    head = raw[:2048].decode('ascii', errors='ignore')
    m = re.search(r'charset=([A-Za-z0-9_\-]+)', head, re.IGNORECASE)
    hint = m.group(1).lower() if m else None
    candidates = []
    if hint:
        candidates.append(hint)
    for c in ['utf-8','cp932','shift_jis','euc_jp']:
        if c not in candidates:
            candidates.append(c)
    for enc in candidates:
        try:
            return raw.decode(enc)
        except Exception:
            pass
    return raw.decode('utf-8', errors='replace')

def normalize_meta_charset(html: str) -> str:
    html2 = re.sub(r'charset=([A-Za-z0-9_\-]+)', 'charset=UTF-8', html, flags=re.IGNORECASE)
    if 'charset=' not in html2.lower():
        html2 = re.sub(r'<head(.*?)>', r'<head\1><meta charset="UTF-8">', html2, count=1, flags=re.IGNORECASE)
    return html2

manifest_rows = []  # populated during fetch actions

def action_fetch_xhtml(target, source, env):
    data = source[0].get_value()  # dict {'card_url','title_guess'}
    card_url = data['card_url']
    title_guess = data['title_guess']
    time.sleep(REQUEST_DELAY_SEC)
    title_card, xhtml_url, enc_hint = extract_xhtml_from_card(card_url)
    if not xhtml_url:
        return None
    r = requests.get(xhtml_url, headers={"User-Agent": USER_AGENT}, timeout=60)
    r.raise_for_status()
    decoded = decode_html(r.content)
    decoded = normalize_meta_charset(decoded)
    final_title = fix_mojibake(title_card or title_guess)
    Path(str(target[0])).write_text(decoded, encoding='utf-8')
    m = re.search(r'card(\d+)\.html', card_url)
    card_id = m.group(1) if m else 'unknown'
    manifest_rows.append([card_id, final_title, xhtml_url, str(target[0]), enc_hint or ''])
    return None

FetchXHTML = Builder(action=Action(action_fetch_xhtml, cmdstr="CARD ${SOURCE} -> ${TARGET}"))
env.Append(BUILDERS={'FetchXHTML': FetchXHTML})

# Schedule XHTML targets if metadata already exists at parse time
if METADATA_JSON.exists():
    try:
        meta_data = json.loads(METADATA_JSON.read_text(encoding='utf-8'))
    except Exception:
        meta_data = []
else:
    meta_data = []

if MAX_ITEMS>0 and meta_data:
    meta_data = meta_data[:MAX_ITEMS]

xhtml_targets = []
for row in meta_data:
    card_url = row['card_url']
    title_guess = row['title_guess']
    m = re.search(r'card(\d+)\.html', card_url)
    card_id = m.group(1) if m else 'unknown'
    base = f"{card_id}_{slugify(fix_mojibake(title_guess))}.html"
    tgt_path = OUT_XHTML_DIR / base
    ensure_dirs()
    tgt = env.FetchXHTML(str(tgt_path), env.Value({'card_url': card_url, 'title_guess': title_guess}))
    env.Depends(tgt, Metadata)
    xhtml_targets.append(tgt)

def action_write_manifest(target, source, env):
    ensure_dirs()
    with MANIFEST.open('w', newline='', encoding='utf-8') as f:
        w = csv.writer(f)
        w.writerow(["card_id","title","xhtml_url","saved_path","encoding_hint"])
        for row in manifest_rows:
            w.writerow(row)
    return None

Manifest = env.Command(str(MANIFEST), xhtml_targets, Action(action_write_manifest, cmdstr="write manifest")) if xhtml_targets else []

Default([AuthorPage, Metadata] + xhtml_targets + ([Manifest] if Manifest else []))

print(f"Phase summary: author_page={'yes' if RAW_AUTHOR.exists() else 'no'}, metadata_entries={len(meta_data)}, xhtml_targets={len(xhtml_targets)}")
if LIST_ONLY:
    if not meta_data:
        print("LIST=1: No metadata yet. Run once without LIST=1 to generate metadata.")
    else:
        print("Planned XHTML targets:")
        for t in xhtml_targets:
            print(f"  {t[0]}")
