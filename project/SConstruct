# SConstruct
# Fetch all Osamu Dazai (person 35) works as XHTML from Aozora Bunko.
# Outputs to out/xhtml/*.html and out/manifest.csv

import os, time, re, json, csv
from pathlib import Path
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

# ---------------- Config ----------------
AUTHOR_PAGE = "https://www.aozora.gr.jp/index_pages/person35.html"
OUT_XHTML_RAW_DIR = Path("out/xhtml_raw")  # newly added raw (original encoding) files
OUT_XHTML_DIR = Path("out/xhtml")          # UTF-8 normalized outputs
METADATA_JSON = Path("out/cards_metadata.json")
MANIFEST = Path("out/manifest.csv")
REQUEST_DELAY_SEC = float(os.environ.get("REQUEST_DELAY_SEC", "0.6"))
USER_AGENT = "SCons-Dazai-XHTML/2.0 (+academic use)"
LIST_ONLY = os.environ.get("LIST") == "1"  # LIST=1 to list targets without downloading

try:
    MAX_ITEMS = int(os.environ.get("MAX_ITEMS", "0"))
except ValueError:
    MAX_ITEMS = 0

env = Environment(ENV={'PATH': os.environ.get('PATH','')})

def ensure_dirs():
    for p in [OUT_XHTML_RAW_DIR, OUT_XHTML_DIR, MANIFEST.parent]:
        p.mkdir(parents=True, exist_ok=True)

def slugify(s: str) -> str:
    s = (s or '').strip()
    s = re.sub(r'\s+', '_', s)
    s = re.sub(r'[^\w\- _\u3040-\u30ff\u4e00-\u9faf]', '', s)
    return s[:60] if len(s) > 60 else (s or 'untitled')

def fix_mojibake(s: str) -> str:
    if not s:
        return s
    def has_jp(txt):
        return bool(re.search(r'[\u3040-\u30FF\u4E00-\u9FAF]', txt))
    if has_jp(s):
        return s
    cands = []
    for enc_from, enc_to in [('latin-1','utf-8'), ('latin-1','cp932'), ('cp932','utf-8')]:
        try:
            cands.append(s.encode(enc_from).decode(enc_to))
        except Exception:
            pass
    for c in cands:
        if has_jp(c):
            return c
    return s

# -------- Phase 1: Fetch author page (parse time) with local cache & build metadata list --------
CACHE_DIR = Path("out/cache")
CACHE_AUTHOR = CACHE_DIR / "person35.html"
REFRESH_AUTHOR = os.environ.get("REFRESH_AUTHOR") == "1"

def fetch_author_html():
    if CACHE_AUTHOR.exists() and not REFRESH_AUTHOR:
        try:
            return CACHE_AUTHOR.read_text(encoding='utf-8', errors='ignore'), True
        except Exception:
            pass
    print("[parse] Downloading author page ...")
    r = requests.get(AUTHOR_PAGE, headers={"User-Agent": USER_AGENT}, timeout=60)
    r.raise_for_status()
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    try:
        CACHE_AUTHOR.write_text(r.text, encoding='utf-8')
    except Exception:
        pass
    return r.text, False

_author_html, _from_cache = fetch_author_html()
print(f"[parse] Author page source: {'cache' if _from_cache else 'network'}")

_soup = BeautifulSoup(_author_html, 'html.parser')
_rows = []
for a in _soup.select('a[href*="/cards/000035/card"]'):
    href = a.get('href')
    if not href:
        continue
    card_url = urljoin(AUTHOR_PAGE, href)
    title = a.get_text(strip=True)
    _rows.append({'title_guess': title, 'card_url': card_url})
_seen = set(); _uniq=[]
for r in _rows:
    if r['card_url'] not in _seen:
        _uniq.append(r); _seen.add(r['card_url'])
if MAX_ITEMS>0:
    _uniq = _uniq[:MAX_ITEMS]
metadata_list = _uniq

# Builder to write metadata JSON from parse-time list
def action_write_metadata(target, source, env):
    data_json = str(source[0])
    Path(str(target[0])).write_text(data_json, encoding='utf-8')
    return None

MetadataBuilder = Builder(action=Action(action_write_metadata, cmdstr="write-metadata -> ${TARGET}"))
env.Append(BUILDERS={'MetadataBuilder': MetadataBuilder})
Metadata = env.MetadataBuilder(str(METADATA_JSON), env.Value(json.dumps(metadata_list, ensure_ascii=False, indent=2)))

# Helper to fetch card page and extract XHTML link
def extract_xhtml_from_card(card_url: str):
    r = requests.get(card_url, headers={"User-Agent": USER_AGENT}, timeout=60)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, 'html.parser')
    title = soup.select_one('h1').get_text(strip=True) if soup.select_one('h1') else ''
    link = None; enc=None
    for tr in soup.find_all('tr'):
        txt = tr.get_text(' ', strip=True)
        if 'XHTML' in txt:
            a = tr.find('a')
            if a and a.get('href') and a.get('href').endswith('.html'):
                link = urljoin(card_url, a['href'])
                m = re.search(r'(ShiftJIS|UTF-8|JIS X 0208)', txt, re.IGNORECASE)
                enc = m.group(1) if m else None
                break
    if not link:
        a = soup.find('a', string=lambda s: s and 'XHTML' in s)
        if a and a.get('href'):
            link = urljoin(card_url, a.get('href'))
    return title, link, enc

def decode_html(raw: bytes) -> str:
    head = raw[:2048].decode('ascii', errors='ignore')
    m = re.search(r'charset=([A-Za-z0-9_\-]+)', head, re.IGNORECASE)
    hint = m.group(1).lower() if m else None
    candidates = []
    if hint:
        candidates.append(hint)
    for c in ['utf-8','cp932','shift_jis','euc_jp']:
        if c not in candidates:
            candidates.append(c)
    for enc in candidates:
        try:
            return raw.decode(enc)
        except Exception:
            pass
    return raw.decode('utf-8', errors='replace')

def normalize_meta_charset(html: str) -> str:
    html2 = re.sub(r'charset=([A-Za-z0-9_\-]+)', 'charset=UTF-8', html, flags=re.IGNORECASE)
    if 'charset=' not in html2.lower():
        html2 = re.sub(r'<head(.*?)>', r'<head\1><meta charset="UTF-8">', html2, count=1, flags=re.IGNORECASE)
    return html2

manifest_rows = []  # populated during conversion actions

def parse_value_dict(val_node):
    raw = str(val_node)
    try:
        if raw.startswith('{'):
            return json.loads(raw.replace("'", '"'))
    except Exception:
        pass
    try:
        import ast
        return ast.literal_eval(raw)
    except Exception:
        raise RuntimeError(f"Unable to parse Value node: {raw}")

def action_fetch_xhtml_raw(target, source, env):
    """Download raw XHTML bytes (no decoding) from card page-discovered link."""
    meta = parse_value_dict(source[0])  # {'card_url','title_guess'}
    card_url = meta['card_url']
    title_guess = meta['title_guess']
    time.sleep(REQUEST_DELAY_SEC)
    title_card, xhtml_url, enc_hint = extract_xhtml_from_card(card_url)
    if not xhtml_url:
        # create empty file to satisfy target to avoid re-fetch loop
        Path(str(target[0])).write_bytes(b"")
        return None
    r = requests.get(xhtml_url, headers={"User-Agent": USER_AGENT}, timeout=60)
    r.raise_for_status()
    Path(str(target[0])).write_bytes(r.content)
    # store lightweight metadata sidecar (encoding hint & better title) for conversion stage
    sidecar = Path(str(target[0]))
    sidecar_meta = sidecar.with_suffix(sidecar.suffix + '.json')
    sidecar_meta.write_text(json.dumps({
        'card_url': card_url,
        'xhtml_url': xhtml_url,
        'title_guess': title_guess,
        'title_card': title_card,
        'enc_hint': enc_hint
    }, ensure_ascii=False, indent=2), encoding='utf-8')
    return None

def action_convert_xhtml(target, source, env):
    """Convert previously downloaded raw XHTML bytes to normalized UTF-8 HTML."""
    raw_file = Path(str(source[0]))
    value_node = source[1]
    meta = parse_value_dict(value_node)
    # prefer sidecar for richer info
    sidecar_meta_path = raw_file.with_suffix(raw_file.suffix + '.json')
    if sidecar_meta_path.exists():
        try:
            meta_sc = json.loads(sidecar_meta_path.read_text(encoding='utf-8'))
            meta.update(meta_sc)
        except Exception:
            pass
    card_url = meta.get('card_url')
    title_guess = meta.get('title_guess')
    title_card = meta.get('title_card')
    enc_hint = meta.get('enc_hint')
    raw = raw_file.read_bytes()
    decoded = decode_html(raw)
    decoded = normalize_meta_charset(decoded)
    final_title = fix_mojibake(title_card or title_guess)
    Path(str(target[0])).write_text(decoded, encoding='utf-8')
    m_id = re.search(r'card(\d+)\.html', card_url or '')
    card_id = m_id.group(1) if m_id else 'unknown'
    manifest_rows.append([card_id, final_title, meta.get('xhtml_url',''), str(target[0]), enc_hint or ''])
    return None

FetchXHTMLRaw = Builder(action=Action(action_fetch_xhtml_raw, cmdstr="RAW ${SOURCE} -> ${TARGET}"))
ConvertXHTML = Builder(action=Action(action_convert_xhtml, cmdstr="CONVERT ${SOURCES} -> ${TARGET}"))
env.Append(BUILDERS={'FetchXHTMLRaw': FetchXHTMLRaw, 'ConvertXHTML': ConvertXHTML})

meta_data = metadata_list  # direct use
raw_targets = []
converted_targets = []
for row in meta_data:
    card_url = row['card_url']
    title_guess = row['title_guess']
    m = re.search(r'card(\d+)\.html', card_url)
    card_id = m.group(1) if m else 'unknown'
    base = f"{card_id}_{slugify(fix_mojibake(title_guess))}.html"
    raw_path = OUT_XHTML_RAW_DIR / base
    final_path = OUT_XHTML_DIR / base
    ensure_dirs()
    val = env.Value({'card_url': card_url, 'title_guess': title_guess})
    raw_tgt = env.FetchXHTMLRaw(str(raw_path), val)
    env.Depends(raw_tgt, Metadata)  # ensure metadata JSON is written first
    conv_tgt = env.ConvertXHTML(str(final_path), [raw_tgt, val])
    env.Depends(conv_tgt, raw_tgt)
    raw_targets.append(raw_tgt)
    converted_targets.append(conv_tgt)

# For compatibility with existing code, treat converted targets as xhtml_targets
xhtml_targets = converted_targets

def action_write_manifest(target, source, env):
    ensure_dirs()
    with MANIFEST.open('w', newline='', encoding='utf-8') as f:
        w = csv.writer(f)
        w.writerow(["card_id","title","xhtml_url","saved_path","encoding_hint"])
        for row in manifest_rows:
            w.writerow(row)
    return None

Manifest = env.Command(str(MANIFEST), xhtml_targets, Action(action_write_manifest, cmdstr="write manifest")) if xhtml_targets else []

Default([Metadata] + xhtml_targets + ([Manifest] if Manifest else []))

print(f"Phase summary: metadata_entries={len(meta_data)}, xhtml_targets={len(xhtml_targets)} (single-pass)")
if LIST_ONLY:
    if not meta_data:
        print("LIST=1: No metadata yet. Run once without LIST=1 to generate metadata.")
    else:
        print("Planned XHTML targets:")
        for t in xhtml_targets:
            print(f"  {t[0]}")
