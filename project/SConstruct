# SConstruct
# Fetch all Osamu Dazai (person 35) works as XHTML from Aozora Bunko.
# Outputs to out/xhtml/*.html and out/manifest.csv

import os, time, re, json, csv
from pathlib import Path
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

# ---------------- Config ----------------
AUTHOR_PAGE = "https://www.aozora.gr.jp/index_pages/person35.html"
OUT_XHTML_RAW_DIR = Path("out/xhtml_raw")  # newly added raw (original encoding) files
OUT_XHTML_DIR = Path("out/xhtml")          # UTF-8 normalized outputs
METADATA_JSON = Path("out/cards_metadata.json")
MANIFEST = Path("out/manifest.csv")
REQUEST_DELAY_SEC = float(os.environ.get("REQUEST_DELAY_SEC", "0.6"))
USER_AGENT = "SCons-Dazai-XHTML/2.0 (+academic use)"

try:
    MAX_ITEMS = int(os.environ.get("MAX_ITEMS", "0"))
except ValueError:
    MAX_ITEMS = 0

env = Environment(ENV={'PATH': os.environ.get('PATH','')})

def ensure_dirs():
    for p in [OUT_XHTML_RAW_DIR, OUT_XHTML_DIR, MANIFEST.parent]:
        p.mkdir(parents=True, exist_ok=True)

def slugify(s: str) -> str:
    s = (s or '').strip()
    s = re.sub(r'\s+', '_', s)
    s = re.sub(r'[^\w\- _\u3040-\u30ff\u4e00-\u9faf]', '', s)
    return s[:60] if len(s) > 60 else (s or 'untitled')

def fix_mojibake(s: str) -> str:
    if not s:
        return s
    def has_jp(txt):
        return bool(re.search(r'[\u3040-\u30FF\u4E00-\u9FAF]', txt))
    if has_jp(s):
        return s
    cands = []
    for enc_from, enc_to in [('latin-1','utf-8'), ('latin-1','cp932'), ('cp932','utf-8')]:
        try:
            cands.append(s.encode(enc_from).decode(enc_to))
        except Exception:
            pass
    for c in cands:
        if has_jp(c):
            return c
    return s

# -------- Phase 1: Fetch author page (parse time) with local cache & build metadata list --------
CACHE_DIR = Path("out/cache")
CACHE_AUTHOR = CACHE_DIR / "person35.html"
REFRESH_AUTHOR = os.environ.get("REFRESH_AUTHOR") == "1"

def fetch_author_html() -> tuple[str, bool]:
    _from_cache = False
    if CACHE_AUTHOR.exists() and not REFRESH_AUTHOR:
        bytes = CACHE_AUTHOR.read_bytes()
        _from_cache = True

    print("[parse] Downloading author page ...")
    r = requests.get(AUTHOR_PAGE, headers={"User-Agent": USER_AGENT}, timeout=60)
    r.raise_for_status()
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    CACHE_AUTHOR.write_bytes(r.content)
    
    bytes = r.content
    
    html_in_utf8: str = bytes.decode("utf-8")
    
    return html_in_utf8, _from_cache

_author_html, _from_cache = fetch_author_html()
print(f"[parse] Author page source: {'cache' if _from_cache else 'network'}")

_soup = BeautifulSoup(_author_html, 'html.parser')
_rows = []
for a in _soup.select('a[href*="/cards/000035/card"]'):
    href = a.get('href')
    if not href:
        continue
    card_url = urljoin(AUTHOR_PAGE, href)
    # Skip card 2276 (confirmed dead link; it's the English translation "A Tale of Honorable Poverty")
    if 'card2276.' in card_url:
        print('[warn] Skipping known broken card: 2276 ->', card_url)
        continue
    # Raw text (BeautifulSoup already decoded page as UTF-8, but title text may carry mojibake if page misdeclared)
    title_raw = a.get_text(strip=True)
    _rows.append({'title_guess': title_raw, 'card_url': card_url})
_seen = set(); _uniq=[]
for r in _rows:
    if r['card_url'] not in _seen:
        _uniq.append(r); _seen.add(r['card_url'])
if MAX_ITEMS>0:
    _uniq = _uniq[:MAX_ITEMS]
metadata_list = _uniq

# Builder to write metadata JSON from parse-time list
def action_write_metadata(target, source, env):
    data_json = str(source[0])
    Path(str(target[0])).write_text(data_json, encoding='utf-8')
    return None

MetadataBuilder = Builder(action=Action(action_write_metadata, cmdstr="write-metadata -> ${TARGET}"))
env.Append(BUILDERS={'MetadataBuilder': MetadataBuilder})
Metadata = env.MetadataBuilder(str(METADATA_JSON), env.Value(json.dumps(metadata_list, ensure_ascii=False, indent=2)))

# Helper to fetch card page and extract XHTML link
def extract_xhtml_from_card(card_url: str):
    r = requests.get(card_url, headers={"User-Agent": USER_AGENT}, timeout=60)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, 'html.parser')
    title = soup.select_one('h1').get_text(strip=True) if soup.select_one('h1') else ''
    link = None; enc=None
    for tr in soup.find_all('tr'):
        txt = tr.get_text(' ', strip=True)
        if 'XHTML' in txt:
            a = tr.find('a')
            if a and a.get('href') and a.get('href').endswith('.html'):
                link = urljoin(card_url, a['href'])
                m = re.search(r'(ShiftJIS|UTF-8|JIS X 0208)', txt, re.IGNORECASE)
                enc = m.group(1) if m else None
                break
    if not link:
        a = soup.find('a', string=lambda s: s and 'XHTML' in s)
        if a and a.get('href'):
            link = urljoin(card_url, a.get('href'))
    return title, link, enc

def decode_html(raw: bytes) -> str:
    head = raw[:2048].decode('ascii', errors='ignore')
    m = re.search(r'charset=([A-Za-z0-9_\-]+)', head, re.IGNORECASE)
    hint = m.group(1).lower() if m else None
    candidates = []
    if hint:
        candidates.append(hint)
    for c in ['utf-8','cp932','shift_jis','euc_jp']:
        if c not in candidates:
            candidates.append(c)
    for enc in candidates:
        try:
            return raw.decode(enc)
        except Exception:
            pass
    return raw.decode('utf-8', errors='replace')

def normalize_meta_charset(html: str) -> str:
    html2 = re.sub(r'charset=([A-Za-z0-9_\-]+)', 'charset=UTF-8', html, flags=re.IGNORECASE)
    if 'charset=' not in html2.lower():
        html2 = re.sub(r'<head(.*?)>', r'<head\1><meta charset="UTF-8">', html2, count=1, flags=re.IGNORECASE)
    return html2

manifest_rows = []  # populated during conversion actions

def parse_value_dict(val_node):
    raw = str(val_node)
    try:
        if raw.startswith('{'):
            return json.loads(raw.replace("'", '"'))
    except Exception:
        pass
    try:
        import ast
        return ast.literal_eval(raw)
    except Exception:
        raise RuntimeError(f"Unable to parse Value node: {raw}")

def action_fetch_xhtml_raw(target, source, env):
    """Download raw XHTML bytes (no decoding) from card page-discovered link."""
    meta = parse_value_dict(source[0])  # {'card_url','title_guess'}
    card_url = meta['card_url']
    title_guess = meta['title_guess']
    time.sleep(REQUEST_DELAY_SEC)
    title_card, xhtml_url, enc_hint = extract_xhtml_from_card(card_url)
    if not xhtml_url:
        msg = f"No XHTML link found for card page: {card_url} (parsed title='{title_card}')"
        raise RuntimeError(msg)
    r = requests.get(xhtml_url, headers={"User-Agent": USER_AGENT}, timeout=60)
    r.raise_for_status()
    Path(str(target[0])).write_bytes(r.content)
    # store lightweight metadata sidecar (encoding hint & better title) for conversion stage
    sidecar = Path(str(target[0]))
    sidecar_meta = sidecar.with_suffix(sidecar.suffix + '.json')
    sidecar_meta.write_text(json.dumps({
        'card_url': card_url,
        'xhtml_url': xhtml_url,
        'title_guess': title_guess,
        'title_card': title_card,
        'enc_hint': enc_hint
    }, ensure_ascii=False, indent=2), encoding='utf-8')
    return None

def action_convert_xhtml(target, source, env):
    """Convert previously downloaded raw XHTML bytes to normalized UTF-8 HTML."""
    raw_file = Path(str(source[0]))
    value_node = source[1]
    meta = parse_value_dict(value_node)
    # prefer sidecar for richer info
    sidecar_meta_path = raw_file.with_suffix(raw_file.suffix + '.json')
    if sidecar_meta_path.exists():
        try:
            meta_sc = json.loads(sidecar_meta_path.read_text(encoding='utf-8'))
            meta.update(meta_sc)
        except Exception:
            pass
    card_url = meta.get('card_url')
    title_guess = meta.get('title_guess')
    title_card = meta.get('title_card')
    enc_hint = meta.get('enc_hint')
    # Guard: raw file must not be empty
    raw = raw_file.read_bytes()
    if len(raw) == 0:
        raise RuntimeError(f"Raw XHTML file is empty: {raw_file}")
    decoded = decode_html(raw)
    decoded = normalize_meta_charset(decoded)
    # Normalize XML declaration encoding to UTF-8 if present
    # We avoid brittle regex for entire document by only touching the XML declaration line if it exists.
    if decoded.lstrip().startswith('<?xml'):
        # Replace only encoding attribute inside declaration
        first_line, *rest = decoded.splitlines(keepends=True)
        first_line_new = re.sub(r'encoding\s*=\s*"[^"]+"', 'encoding="UTF-8"', first_line, flags=re.IGNORECASE)
        if first_line != first_line_new:
            decoded = first_line_new + ''.join(rest)
        # If no encoding attr, insert one (rare for these docs)
        elif 'encoding' not in first_line.lower():
            first_line_new = first_line.rstrip('?>') + ' encoding="UTF-8"?>'
            decoded = first_line_new + ''.join(rest)
    final_title = fix_mojibake(title_card or title_guess)
    Path(str(target[0])).write_text(decoded, encoding='utf-8')
    m_id = re.search(r'card(\d+)\.html', card_url or '')
    card_id = m_id.group(1) if m_id else 'unknown'
    manifest_rows.append([card_id, final_title, meta.get('xhtml_url',''), str(target[0]), enc_hint or ''])
    return None

FetchXHTMLRaw = Builder(action=Action(action_fetch_xhtml_raw, cmdstr="RAW ${SOURCE} -> ${TARGET}"))
ConvertXHTML = Builder(action=Action(action_convert_xhtml, cmdstr="CONVERT ${SOURCES} -> ${TARGET}"))
env.Append(BUILDERS={'FetchXHTMLRaw': FetchXHTMLRaw, 'ConvertXHTML': ConvertXHTML})

meta_data = metadata_list  # direct use
raw_targets = []
converted_targets = []
for row in meta_data:
    card_url = row['card_url']
    title_guess = row['title_guess']
    m = re.search(r'card(\d+)\.html', card_url)
    card_id = m.group(1) if m else 'unknown'
    base = f"{card_id}_{slugify(fix_mojibake(title_guess))}.html"
    raw_path = OUT_XHTML_RAW_DIR / base
    final_path = OUT_XHTML_DIR / base
    ensure_dirs()
    val = env.Value({'card_url': card_url, 'title_guess': title_guess})
    raw_tgt = env.FetchXHTMLRaw(str(raw_path), val)
    env.Depends(raw_tgt, Metadata)  # ensure metadata JSON is written first
    conv_tgt = env.ConvertXHTML(str(final_path), [raw_tgt, val])
    env.Depends(conv_tgt, raw_tgt)
    raw_targets.append(raw_tgt)
    converted_targets.append(conv_tgt)

# For compatibility with existing code, treat converted targets as xhtml_targets
xhtml_targets = converted_targets

def action_write_manifest(target, source, env):
    ensure_dirs()
    with MANIFEST.open('w', newline='', encoding='utf-8') as f:
        w = csv.writer(f)
        w.writerow(["card_id","title","xhtml_url","saved_path","encoding_hint"])
        for row in manifest_rows:
            w.writerow(row)
    return None

Manifest = env.Command(str(MANIFEST), xhtml_targets, Action(action_write_manifest, cmdstr="write manifest")) if xhtml_targets else []

# ---------------- Validation: ensure UTF-8 XML declaration ----------------
def action_validate_utf8_decl(target, source, env):
    errors = []
    for s in source:
        p = Path(str(s))
        try:
            first_256 = p.read_text(encoding='utf-8').splitlines()[0:1]
        except Exception as e:
            errors.append(f"{p}: unreadable ({e})")
            continue
        if not first_256:
            errors.append(f"{p}: empty file")
            continue
        line = first_256[0].strip()
        if not (line.startswith('<?xml') and 'encoding="UTF-8"' in line):
            errors.append(f"{p}: bad or missing UTF-8 xml declaration -> {line}")
    if errors:
        # Write a report file for easier inspection
        report_path = Path('out/validation_utf8_decl.txt')
        report_path.parent.mkdir(parents=True, exist_ok=True)
        report_path.write_text('\n'.join(errors), encoding='utf-8')
        print("UTF-8 declaration validation failed; see out/validation_utf8_decl.txt")
        return 1
    Path(str(target[0])).write_text('ok', encoding='utf-8')
    return None

ValidateUTF8Decl = env.Command('out/validate_utf8_decl.ok', xhtml_targets, Action(action_validate_utf8_decl, cmdstr="validate utf8 decl")) if xhtml_targets else []

Default([Metadata] + xhtml_targets + ([Manifest] if Manifest else []) + ([ValidateUTF8Decl] if ValidateUTF8Decl else []))

print(f"Phase summary: metadata_entries={len(meta_data)}, xhtml_targets={len(xhtml_targets)} (single-pass)")